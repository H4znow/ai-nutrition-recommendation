{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic dataset generation for demonstration\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = 10000\n",
    "X_features = []    # user feature vectors\n",
    "Y_meals = []       # ground-truth meal class sequences (length 6 for each user)\n",
    "target_EIs = []    # target daily energy intakes for each user\n",
    "min_macros = []    # minimum recommended macronutrient values for each user (based on guidelines)\n",
    "max_macros = []    # maximum recommended macronutrient values for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_users):\n",
    "    # Random user profile\n",
    "    weight = np.random.uniform(50, 100)   # kg\n",
    "    height = np.random.uniform(150, 200)  # cm\n",
    "    BMI = weight / ((height/100)**2)\n",
    "    age = np.random.randint(18, 60)\n",
    "    # Basal Metabolic Rate (BMR) using Mifflin-St Jeor formula (for a male user as an example)\n",
    "    BMR = 10*weight + 6.25*height - 5*age + 5\n",
    "    PAL = np.random.uniform(1.2, 2.0)     # Physical Activity Level (sedentary ~1.2 to very active ~2.0)\n",
    "    # Medical conditions (binary flags for presence of cardiovascular disease, type-2 diabetes, iron deficiency)\n",
    "    has_CVD = np.random.rand() < 0.1      # 10% chance\n",
    "    has_T2D = np.random.rand() < 0.1      # 10% chance\n",
    "    has_iron_def = np.random.rand() < 0.1 # 10% chance\n",
    "    \n",
    "    # Compose feature vector\n",
    "    user_features = [weight, height, BMI, BMR, PAL, int(has_CVD), int(has_T2D), int(has_iron_def)]\n",
    "    X_features.append(user_features)\n",
    "    \n",
    "    # Determine target daily energy intake using factorial method (BMR * PAL) adjusted by factor D for BMI (based on nutritional guidelines)\n",
    "    D = 1.0\n",
    "    if BMI < 18.5:    # underweight: increase target EI to encourage weight gain\n",
    "        D = 1.1\n",
    "    elif BMI > 25:    # overweight: decrease target EI for weight loss\n",
    "        D = 0.9\n",
    "    target_EI = BMR * PAL * D\n",
    "    target_EIs.append(target_EI)\n",
    "    \n",
    "    # Recommended macronutrient intake ranges (based on nutritional guidelines)\n",
    "    # For simplicity, use fixed percentage ranges of total energy for each macro:\n",
    "    # Protein: 10-35%, Carbs: 45-65%, Fat: 20-35%, SFA: 0-10% of total energy.\n",
    "    min_prot, max_prot = 0.10, 0.35\n",
    "    min_carb, max_carb = 0.45, 0.65\n",
    "    min_fat, max_fat   = 0.20, 0.35\n",
    "    min_sfa, max_sfa   = 0.00, 0.10\n",
    "    # Convert these fractions to absolute amounts (grams) using energy densities (4 kcal/g for protein & carbs, 9 kcal/g for fat & SFA)\n",
    "    min_prot_g = min_prot * target_EI / 4.0;   max_prot_g = max_prot * target_EI / 4.0\n",
    "    min_carb_g = min_carb * target_EI / 4.0;   max_carb_g = max_carb * target_EI / 4.0\n",
    "    min_fat_g  = min_fat  * target_EI / 9.0;   max_fat_g  = max_fat  * target_EI / 9.0\n",
    "    min_sfa_g  = min_sfa  * target_EI / 9.0;   max_sfa_g  = max_sfa  * target_EI / 9.0\n",
    "    min_macros.append([min_prot_g, min_carb_g, min_fat_g, min_sfa_g])\n",
    "    max_macros.append([max_prot_g, max_carb_g, max_fat_g, max_sfa_g])\n",
    "    \n",
    "    # Generate a synthetic \"ground truth\" meal plan (sequence of 6 meal class labels) for the user.\n",
    "    # We bias the meal choices based on user's BMI category for realism:\n",
    "    # Underweight users get more high-calorie meals, overweight get more low-calorie meals.\n",
    "    meal_classes = []\n",
    "    if BMI < 18.5:\n",
    "        # Underweight: 80% chance to pick a high-calorie meal (class 0-4), 20% chance low-calorie (5-9)\n",
    "        for t in range(6):\n",
    "            if np.random.rand() < 0.8:\n",
    "                meal_classes.append(np.random.randint(0, 5))   # high-calorie meal class\n",
    "            else:\n",
    "                meal_classes.append(np.random.randint(5, 10))  # low-calorie meal class\n",
    "    elif BMI > 25:\n",
    "        # Overweight: 80% chance low-calorie meal, 20% high-calorie meal\n",
    "        for t in range(6):\n",
    "            if np.random.rand() < 0.8:\n",
    "                meal_classes.append(np.random.randint(5, 10))  # low-calorie meal class\n",
    "            else:\n",
    "                meal_classes.append(np.random.randint(0, 5))   # high-calorie meal class\n",
    "    else:\n",
    "        # Normal weight: no strong bias, random meals\n",
    "        for t in range(6):\n",
    "            meal_classes.append(np.random.randint(0, 10))\n",
    "    Y_meals.append(meal_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data lists to tensors for training\n",
    "X_features = torch.tensor(X_features, dtype=torch.float32)\n",
    "Y_meals = torch.tensor(Y_meals, dtype=torch.long)\n",
    "target_EIs = torch.tensor(target_EIs, dtype=torch.float32)\n",
    "min_macros = torch.tensor(min_macros, dtype=torch.float32)\n",
    "max_macros = torch.tensor(max_macros, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 8]) torch.Size([10000, 6]) torch.Size([10000]) torch.Size([10000, 4]) torch.Size([10000, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    X_features.shape,\n",
    "    Y_meals.shape,\n",
    "    target_EIs.shape,\n",
    "    min_macros.shape,\n",
    "    max_macros.shape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3462.2251, 3184.7493, 2556.8049,  ..., 2154.7561, 2250.2383,\n",
       "        2484.2988])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_EIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tensors to numpy arrays\n",
    "X_features_np = X_features.numpy()\n",
    "Y_meals_np = Y_meals.numpy()\n",
    "target_EIs_np = target_EIs.numpy()\n",
    "min_macros_np = min_macros.numpy()\n",
    "max_macros_np = max_macros.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\"weight\", \"height\", \"BMI\", \"BMR\", \"PAL\", \"has_CVD\", \"has_T2D\", \"has_iron_def\"]\n",
    "df_features = pd.DataFrame(X_features_np, columns=feature_columns)\n",
    "df_features[\"target_EI\"] = target_EIs_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "meal_columns = [f\"meal_{i+1}\" for i in range(Y_meals_np.shape[1])]\n",
    "df_meals = pd.DataFrame(Y_meals_np, columns=meal_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_macro_columns = [\"min_prot\", \"min_carb\", \"min_fat\", \"min_sfa\"]\n",
    "df_min_macros = pd.DataFrame(min_macros_np, columns=min_macro_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_macro_columns = [\"max_prot\", \"max_carb\", \"max_fat\", \"max_sfa\"]\n",
    "df_max_macros = pd.DataFrame(max_macros_np, columns=max_macro_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight</th>\n",
       "      <th>height</th>\n",
       "      <th>BMI</th>\n",
       "      <th>BMR</th>\n",
       "      <th>PAL</th>\n",
       "      <th>has_CVD</th>\n",
       "      <th>has_T2D</th>\n",
       "      <th>has_iron_def</th>\n",
       "      <th>target_EI</th>\n",
       "      <th>meal_1</th>\n",
       "      <th>...</th>\n",
       "      <th>meal_5</th>\n",
       "      <th>meal_6</th>\n",
       "      <th>min_prot</th>\n",
       "      <th>min_carb</th>\n",
       "      <th>min_fat</th>\n",
       "      <th>min_sfa</th>\n",
       "      <th>max_prot</th>\n",
       "      <th>max_carb</th>\n",
       "      <th>max_fat</th>\n",
       "      <th>max_sfa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77.440674</td>\n",
       "      <td>185.759476</td>\n",
       "      <td>22.442291</td>\n",
       "      <td>1835.403442</td>\n",
       "      <td>1.886356</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3462.225098</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>86.555626</td>\n",
       "      <td>389.500336</td>\n",
       "      <td>76.938339</td>\n",
       "      <td>0.0</td>\n",
       "      <td>302.944702</td>\n",
       "      <td>562.611572</td>\n",
       "      <td>134.642090</td>\n",
       "      <td>38.469170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.608437</td>\n",
       "      <td>173.998856</td>\n",
       "      <td>29.927872</td>\n",
       "      <td>1823.577271</td>\n",
       "      <td>1.940477</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3184.749268</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>79.618729</td>\n",
       "      <td>358.284302</td>\n",
       "      <td>70.772202</td>\n",
       "      <td>0.0</td>\n",
       "      <td>278.665558</td>\n",
       "      <td>517.521729</td>\n",
       "      <td>123.851357</td>\n",
       "      <td>35.386101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70.733093</td>\n",
       "      <td>163.227783</td>\n",
       "      <td>26.548166</td>\n",
       "      <td>1587.504639</td>\n",
       "      <td>1.789535</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2556.804932</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>63.920120</td>\n",
       "      <td>287.640564</td>\n",
       "      <td>56.817886</td>\n",
       "      <td>0.0</td>\n",
       "      <td>223.720428</td>\n",
       "      <td>415.480804</td>\n",
       "      <td>99.431297</td>\n",
       "      <td>28.408943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60.519127</td>\n",
       "      <td>156.446320</td>\n",
       "      <td>24.726463</td>\n",
       "      <td>1427.980713</td>\n",
       "      <td>1.686265</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2407.953369</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>60.198833</td>\n",
       "      <td>270.894745</td>\n",
       "      <td>53.510075</td>\n",
       "      <td>0.0</td>\n",
       "      <td>210.695908</td>\n",
       "      <td>391.292419</td>\n",
       "      <td>93.642632</td>\n",
       "      <td>26.755037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62.664581</td>\n",
       "      <td>173.315536</td>\n",
       "      <td>20.861576</td>\n",
       "      <td>1449.867920</td>\n",
       "      <td>1.698808</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2463.047363</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>61.576183</td>\n",
       "      <td>277.092834</td>\n",
       "      <td>54.734386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>215.516647</td>\n",
       "      <td>400.245178</td>\n",
       "      <td>95.785172</td>\n",
       "      <td>27.367193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      weight      height        BMI          BMR       PAL  has_CVD  has_T2D  \\\n",
       "0  77.440674  185.759476  22.442291  1835.403442  1.886356      0.0      0.0   \n",
       "1  90.608437  173.998856  29.927872  1823.577271  1.940477      1.0      1.0   \n",
       "2  70.733093  163.227783  26.548166  1587.504639  1.789535      0.0      0.0   \n",
       "3  60.519127  156.446320  24.726463  1427.980713  1.686265      0.0      1.0   \n",
       "4  62.664581  173.315536  20.861576  1449.867920  1.698808      0.0      0.0   \n",
       "\n",
       "   has_iron_def    target_EI  meal_1  ...  meal_5  meal_6   min_prot  \\\n",
       "0           0.0  3462.225098       7  ...       1       6  86.555626   \n",
       "1           1.0  3184.749268       0  ...       6       2  79.618729   \n",
       "2           0.0  2556.804932       5  ...       6       3  63.920120   \n",
       "3           0.0  2407.953369       6  ...       4       4  60.198833   \n",
       "4           0.0  2463.047363       5  ...       5       0  61.576183   \n",
       "\n",
       "     min_carb    min_fat  min_sfa    max_prot    max_carb     max_fat  \\\n",
       "0  389.500336  76.938339      0.0  302.944702  562.611572  134.642090   \n",
       "1  358.284302  70.772202      0.0  278.665558  517.521729  123.851357   \n",
       "2  287.640564  56.817886      0.0  223.720428  415.480804   99.431297   \n",
       "3  270.894745  53.510075      0.0  210.695908  391.292419   93.642632   \n",
       "4  277.092834  54.734386      0.0  215.516647  400.245178   95.785172   \n",
       "\n",
       "     max_sfa  \n",
       "0  38.469170  \n",
       "1  35.386101  \n",
       "2  28.408943  \n",
       "3  26.755037  \n",
       "4  27.367193  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full = pd.concat([df_features, df_meals, df_min_macros, df_max_macros], axis=1)\n",
    "df_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_features_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 6)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_meals_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 23 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   weight        10000 non-null  float32\n",
      " 1   height        10000 non-null  float32\n",
      " 2   BMI           10000 non-null  float32\n",
      " 3   BMR           10000 non-null  float32\n",
      " 4   PAL           10000 non-null  float32\n",
      " 5   has_CVD       10000 non-null  float32\n",
      " 6   has_T2D       10000 non-null  float32\n",
      " 7   has_iron_def  10000 non-null  float32\n",
      " 8   target_EI     10000 non-null  float32\n",
      " 9   meal_1        10000 non-null  int64  \n",
      " 10  meal_2        10000 non-null  int64  \n",
      " 11  meal_3        10000 non-null  int64  \n",
      " 12  meal_4        10000 non-null  int64  \n",
      " 13  meal_5        10000 non-null  int64  \n",
      " 14  meal_6        10000 non-null  int64  \n",
      " 15  min_prot      10000 non-null  float32\n",
      " 16  min_carb      10000 non-null  float32\n",
      " 17  min_fat       10000 non-null  float32\n",
      " 18  min_sfa       10000 non-null  float32\n",
      " 19  max_prot      10000 non-null  float32\n",
      " 20  max_carb      10000 non-null  float32\n",
      " 21  max_fat       10000 non-null  float32\n",
      " 22  max_sfa       10000 non-null  float32\n",
      "dtypes: float32(17), int64(6)\n",
      "memory usage: 1.1 MB\n"
     ]
    }
   ],
   "source": [
    "df_full.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.to_csv(\"synthetic_nutrition_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        # fc1: projects input features to a hidden representation.\n",
    "        #   Input: [batch_size, input_dim] → Output: [batch_size, hidden_dim]\n",
    "        self.fc1 = nn.Linear(in_features=input_dim, out_features=hidden_dim)\n",
    "        \n",
    "        # fc2: further processes the hidden representation.\n",
    "        #   Input: [batch_size, hidden_dim] → Output: [batch_size, hidden_dim]\n",
    "        self.fc2 = nn.Linear(in_features=hidden_dim, out_features=hidden_dim)\n",
    "\n",
    "        # fc_mu: computes the mean of the latent distribution.\n",
    "        #   Input: [batch_size, hidden_dim] → Output: [batch_size, latent_dim]\n",
    "        self.fc_mu = nn.Linear(in_features=hidden_dim, out_features=latent_dim)\n",
    "        \n",
    "        # fc_logvar: computes the log-variance of the latent distribution.\n",
    "        #   Input: [batch_size, hidden_dim] → Output: [batch_size, latent_dim]\n",
    "        self.fc_logvar = nn.Linear(in_features=hidden_dim, out_features=latent_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply fc1 with ReLU activation.\n",
    "        #   x: [batch_size, input_dim] → [batch_size, hidden_dim]\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        # Apply fc2 with ReLU activation.\n",
    "        #   x: [batch_size, hidden_dim] → h: [batch_size, hidden_dim]\n",
    "        h = F.relu(self.fc2(x))\n",
    "        \n",
    "        # Compute the latent mean.\n",
    "        #   h: [batch_size, hidden_dim] → mu: [batch_size, latent_dim]\n",
    "        mu = self.fc_mu(h)\n",
    "        \n",
    "        # Compute the latent log-variance.\n",
    "        #   h: [batch_size, hidden_dim] → logvar: [batch_size, latent_dim]\n",
    "        logvar = self.fc_logvar(h)\n",
    "        \n",
    "        return mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 8\n",
    "hidden_units = 62\n",
    "latent_dim = 16\n",
    "\n",
    "encoder = Encoder(input_dim, hidden_units, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #\n",
       "===================================================================================================================\n",
       "Encoder                                  [8]                       [16]                      --\n",
       "├─Linear: 1-1                            [8]                       [62]                      558\n",
       "├─Linear: 1-2                            [62]                      [62]                      3,906\n",
       "├─Linear: 1-3                            [62]                      [16]                      1,008\n",
       "├─Linear: 1-4                            [62]                      [16]                      1,008\n",
       "===================================================================================================================\n",
       "Total params: 6,480\n",
       "Trainable params: 6,480\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.31\n",
       "===================================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.03\n",
       "Estimated Total Size (MB): 0.03\n",
       "==================================================================================================================="
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(encoder, (input_dim,), col_names=[\"input_size\", \"output_size\", \"num_params\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_units, num_classes, macro_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_units = hidden_units\n",
    "        self.macro_dim = macro_dim\n",
    "\n",
    "        # Projects latent vector (shape: [batch_size, latent_dim]) to hidden space (shape: [batch_size, hidden_dim])\n",
    "        self.latent_to_hidden = nn.Linear(in_features=latent_dim, out_features=hidden_units)\n",
    "\n",
    "        # GRUCell that takes an input of shape [batch_size, hidden_dim] and outputs a hidden state of the same shape\n",
    "        self.gru1 = nn.GRUCell(input_size=hidden_units, hidden_size=hidden_units)\n",
    "        self.gru2 = nn.GRUCell(input_size=hidden_units, hidden_size=hidden_units)\n",
    "\n",
    "        # Classifier head: maps hidden state [batch_size, hidden_dim] to class logits [batch_size, num_classes]\n",
    "        self.classifier = nn.Linear(in_features=hidden_units, out_features=num_classes)\n",
    "        # Energy head: maps hidden state [batch_size, hidden_dim] to a scalar energy [batch_size, 1]\n",
    "        self.energy_head = nn.Linear(in_features=hidden_units, out_features=1)\n",
    "        # Macro head: maps hidden state [batch_size, hidden_dim] to macro outputs [batch_size, macro_dim]\n",
    "        self.macro_head = nn.Linear(in_features=hidden_units, out_features=macro_dim) \n",
    "    \n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            z (torch.Tensor): Latent vector of shape [batch_size, latent_dim]\n",
    "            \n",
    "        Returns:\n",
    "            class_logits_seq (torch.Tensor): Sequence of class logits, shape [batch_size, T, num_classes]\n",
    "            total_energy (torch.Tensor): Summed energy over T time steps, shape [batch_size, 1]\n",
    "            total_macros (torch.Tensor): Accumulated macro outputs over T time steps, shape [batch_size, macro_dim]\n",
    "            energies_tensor (torch.Tensor): Sequence of energy values, shape [batch_size, T, 1]\n",
    "        \"\"\"\n",
    "        batch_size = z.size(0)\n",
    "        T = 6  # Number of GRU time steps\n",
    "\n",
    "\n",
    "        # Initialize hidden state for GRUCell with zeros, shape: [batch_size, hidden_dim]\n",
    "        h1 = torch.zeros(size=(batch_size, self.hidden_units), device=z.device)\n",
    "        h2 = torch.zeros(size=(batch_size, self.hidden_units), device=z.device)\n",
    "        h_prev = h2\n",
    "\n",
    "        # Project latent vector to hidden space (input for GRU at t=0), shape: [batch_size, hidden_dim]\n",
    "        z_projected = self.latent_to_hidden(z)\n",
    "         \n",
    "        class_logits_seq = []\n",
    "        energies_list = []\n",
    "        # Initialize accumulation for macro outputs, shape: [batch_size, macro_dim]\n",
    "        total_macros = torch.zeros(batch_size, self.macro_dim, device=z.device)\n",
    "\n",
    "        for t in range(T):\n",
    "            # For t=0, use the projected latent vector; for t>0, use previous hidden state as input.\n",
    "            if t == 0:\n",
    "                z = z_projected\n",
    "            else:\n",
    "                z = h_prev\n",
    "\n",
    "            # GRUCell update: input z and previous hidden state h, both of shape [batch_size, hidden_dim]\n",
    "            h1 = self.gru1(z, h1)\n",
    "            h2 = self.gru2(h1, h2)\n",
    "\n",
    "            # Compute outputs from the current hidden state\n",
    "            logits = self.classifier(h2)    # Shape: [batch_size, num_classes]\n",
    "            energy = self.energy_head(h2)     # Shape: [batch_size, 1]\n",
    "            macros = self.macro_head(h2)      # Shape: [batch_size, macro_dim]\n",
    "\n",
    "            class_logits_seq.append(logits)\n",
    "            energies_list.append(energy)\n",
    "            total_macros += macros  # Accumulate macro outputs over time steps\n",
    "\n",
    "            h_prev = h2  # Save current hidden state for next iteration\n",
    "\n",
    "        # Stack list of tensors along a new time dimension: [batch_size, T, num_classes] and [batch_size, T, 1]\n",
    "        class_logits_seq = torch.stack(class_logits_seq, dim=1)\n",
    "        energies_tensor = torch.stack(energies_list, dim=1)\n",
    "        # Sum energy values over the time dimension, resulting in shape: [batch_size, 1]\n",
    "        total_energy = energies_tensor.sum(dim=1)\n",
    "\n",
    "        return class_logits_seq, total_energy, total_macros, energies_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 16\n",
    "hidden_units = 25\n",
    "num_classes = 140\n",
    "macro_dim = 5\n",
    "batch_size = 50\n",
    "\n",
    "decoder = Decoder(latent_dim, hidden_units, num_classes, macro_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #\n",
       "===================================================================================================================\n",
       "Decoder                                  [50, 16]                  [50, 6, 140]              --\n",
       "├─Linear: 1-1                            [50, 16]                  [50, 25]                  425\n",
       "├─GRUCell: 1-2                           [50, 25]                  [50, 25]                  3,900\n",
       "├─GRUCell: 1-3                           [50, 25]                  [50, 25]                  3,900\n",
       "├─Linear: 1-4                            [50, 25]                  [50, 140]                 3,640\n",
       "├─Linear: 1-5                            [50, 25]                  [50, 1]                   26\n",
       "├─Linear: 1-6                            [50, 25]                  [50, 5]                   130\n",
       "├─GRUCell: 1-7                           [50, 25]                  [50, 25]                  (recursive)\n",
       "├─GRUCell: 1-8                           [50, 25]                  [50, 25]                  (recursive)\n",
       "├─Linear: 1-9                            [50, 25]                  [50, 140]                 (recursive)\n",
       "├─Linear: 1-10                           [50, 25]                  [50, 1]                   (recursive)\n",
       "├─Linear: 1-11                           [50, 25]                  [50, 5]                   (recursive)\n",
       "├─GRUCell: 1-12                          [50, 25]                  [50, 25]                  (recursive)\n",
       "├─GRUCell: 1-13                          [50, 25]                  [50, 25]                  (recursive)\n",
       "├─Linear: 1-14                           [50, 25]                  [50, 140]                 (recursive)\n",
       "├─Linear: 1-15                           [50, 25]                  [50, 1]                   (recursive)\n",
       "├─Linear: 1-16                           [50, 25]                  [50, 5]                   (recursive)\n",
       "├─GRUCell: 1-17                          [50, 25]                  [50, 25]                  (recursive)\n",
       "├─GRUCell: 1-18                          [50, 25]                  [50, 25]                  (recursive)\n",
       "├─Linear: 1-19                           [50, 25]                  [50, 140]                 (recursive)\n",
       "├─Linear: 1-20                           [50, 25]                  [50, 1]                   (recursive)\n",
       "├─Linear: 1-21                           [50, 25]                  [50, 5]                   (recursive)\n",
       "├─GRUCell: 1-22                          [50, 25]                  [50, 25]                  (recursive)\n",
       "├─GRUCell: 1-23                          [50, 25]                  [50, 25]                  (recursive)\n",
       "├─Linear: 1-24                           [50, 25]                  [50, 140]                 (recursive)\n",
       "├─Linear: 1-25                           [50, 25]                  [50, 1]                   (recursive)\n",
       "├─Linear: 1-26                           [50, 25]                  [50, 5]                   (recursive)\n",
       "├─GRUCell: 1-27                          [50, 25]                  [50, 25]                  (recursive)\n",
       "├─GRUCell: 1-28                          [50, 25]                  [50, 25]                  (recursive)\n",
       "├─Linear: 1-29                           [50, 25]                  [50, 140]                 (recursive)\n",
       "├─Linear: 1-30                           [50, 25]                  [50, 1]                   (recursive)\n",
       "├─Linear: 1-31                           [50, 25]                  [50, 5]                   (recursive)\n",
       "===================================================================================================================\n",
       "Total params: 12,021\n",
       "Trainable params: 12,021\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 59.66\n",
       "===================================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.48\n",
       "Params size (MB): 0.05\n",
       "Estimated Total Size (MB): 0.53\n",
       "==================================================================================================================="
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(decoder, (batch_size, latent_dim), col_names=[\"input_size\", \"output_size\", \"num_params\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_meal_quantity(energies, batch_target_EI):\n",
    "    pred_total_energy = energies.sum(dim=1)\n",
    "    d = (target_EI - pred_total_energy) / pred_total_energy\n",
    "    d_expanded = d.unsqueeze(1)\n",
    "    adjusted_energies = energies * (1 + d_expanded)\n",
    "    new_total_energy = adjusted_energies.sum(dim=1)\n",
    "    return adjusted_energies, new_total_energy   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_L_macro(batch_min_macros, batch_max_macros, pred_macros):\n",
    "    diff_min = torch.abs(batch_min_macros - pred_macros)\n",
    "    diff_max = torch.abs(batch_max_macros - pred_macros)\n",
    "    macro_penalty = diff_min + diff_max\n",
    "    L_macro = macro_penalty.mean()\n",
    "    return L_macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_L_energy(pred_energy, batch_target_EI):\n",
    "    L_energy = F.mse_loss(pred_energy, batch_target_EI)\n",
    "    return L_energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_KLD(mu, logvar, batch_size):\n",
    "    KLD_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    KLD_loss = KLD_loss / batch_size\n",
    "    return KLD_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_L_MC(class_logits, batch_Y):\n",
    "    T = class_logits.size(1)\n",
    "    CE_loss = 0.0\n",
    "    for t in range(T):\n",
    "        CE_loss += F.cross_entropy(class_logits[:, t, :], batch_Y[:, t])\n",
    "    CE_loss = CE_loss / T\n",
    "    return CE_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(encoder, decoder, X_features, Y_meals, target_EIs, min_macros, max_macros, batch_size, optimizer):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    # X_features: [num_samples, feature_dim]\n",
    "    # Y_meals: [num_samples, num_meals]\n",
    "    # target_EIs: [num_samples, 1]\n",
    "    # min_macros and max_macros: [num_samples, macro_dim]\n",
    "    num_samples = X_features.size(0)\n",
    "    permutation = torch.randperm(num_samples)  # permutation: [num_samples]\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        indices = permutation[i: i+batch_size]      # indices: [batch_size] (or fewer for the last batch)\n",
    "        batch_X = X_features[indices]                 # batch_X: [batch_size, feature_dim]\n",
    "        batch_Y = Y_meals[indices]                    # batch_Y: [batch_size, num_meals]\n",
    "        batch_target_EI = target_EIs[indices]         # batch_target_EI: [batch_size, 1]\n",
    "        batch_min_macros = min_macros[indices]          # batch_min_macros: [batch_size, macro_dim]\n",
    "        batch_max_macros = max_macros[indices]          # batch_max_macros: [batch_size, macro_dim]\n",
    "\n",
    "        # Forward pass through the encoder:\n",
    "        # mu and logvar each have shape [batch_size, latent_dim]\n",
    "        mu, logvar = encoder(batch_X)\n",
    "        epsilon = torch.randn_like(logvar)            # epsilon: [batch_size, latent_dim]\n",
    "        z = mu + epsilon * logvar                     # z: [batch_size, latent_dim]\n",
    "\n",
    "        # Forward pass through the decoder:\n",
    "        # class_logits: [batch_size, T, num_classes] where T is the number of time steps (here T = 6)\n",
    "        # pred_energy: [batch_size, 1]\n",
    "        # pred_macros: [batch_size, macro_dim]\n",
    "        # energies_tensor: [batch_size, T, 1]\n",
    "        class_logits, pred_energy, pred_macros, energies_tensor = decoder(z)\n",
    "\n",
    "        # Compute losses:\n",
    "        # L_macro: scalar loss related to macro constraints\n",
    "        L_macro = compute_L_macro(batch_min_macros, batch_max_macros, pred_macros)\n",
    "        # L_energy: scalar loss comparing predicted energy to target energy\n",
    "        L_energy = compute_L_energy(pred_energy, batch_target_EI)\n",
    "        # L_kld: Kullback-Leibler divergence loss, scalar\n",
    "        L_kld = compute_KLD(mu, logvar, batch_size)\n",
    "        # L_mc: classification loss computed from the logits over T time steps\n",
    "        L_mc = compute_L_MC(class_logits, batch_Y)\n",
    "        \n",
    "        loss = L_macro + L_energy + L_kld + L_mc\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    avg_loss = total_loss / (num_samples / batch_size)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(csv_file, batch_size=64, hidden_dim=256, latent_dim=256, hidden_units=512, epochs=500):\n",
    "    # Load the CSV file into a pandas DataFrame\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    feature_columns = [\"weight\", \"height\", \"BMI\", \"BMR\", \"PAL\", \"has_CVD\", \"has_T2D\", \"has_iron_def\"]\n",
    "    X_features_np = df[feature_columns].values\n",
    "    target_EIs_np = df[\"target_EI\"].values\n",
    "    \n",
    "    # Deduce meal labels (6 meals per day)\n",
    "    meal_columns = [f\"meal_{i+1}\" for i in range(6)]\n",
    "    Y_meals_np = df[meal_columns].values\n",
    "    \n",
    "    # Deduce macronutrient ranges\n",
    "    min_macro_columns = [\"min_prot\", \"min_carb\", \"min_fat\", \"min_sfa\"]\n",
    "    max_macro_columns = [\"max_prot\", \"max_carb\", \"max_fat\", \"max_sfa\"]\n",
    "    min_macros_np = df[min_macro_columns].values\n",
    "    max_macros_np = df[max_macro_columns].values\n",
    "\n",
    "    # Convert arrays to PyTorch tensors\n",
    "    X_features = torch.tensor(X_features_np, dtype=torch.float32)\n",
    "    Y_meals = torch.tensor(Y_meals_np, dtype=torch.long)\n",
    "    target_EIs = torch.tensor(target_EIs_np, dtype=torch.float32)\n",
    "    min_macros = torch.tensor(min_macros_np, dtype=torch.float32)\n",
    "    max_macros = torch.tensor(max_macros_np, dtype=torch.float32)\n",
    "\n",
    "    input_dim = X_features.shape[1]                # Number of user features (should be 8)\n",
    "    num_classes = int(Y_meals.max().item() + 1)      # Meal classes, deduced from the maximum label + 1\n",
    "    macro_dim = min_macros.shape[1]                  # Number of macronutrients (should be 4)\n",
    "    \n",
    "    print(f\"Deduced dimensions: input_dim={input_dim}, num_classes={num_classes}, macro_dim={macro_dim}\")\n",
    "    \n",
    "    # Instantiate Variation AutoEncoder\n",
    "    encoder = Encoder(input_dim=input_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)\n",
    "    decoder = Decoder(latent_dim=latent_dim, hidden_units=hidden_units, num_classes=num_classes, macro_dim=macro_dim)\n",
    "    \n",
    "    # Setup the optimizer (cf. paper)\n",
    "    optimizer = Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=1e-4)\n",
    "    \n",
    "    # Training loop\n",
    "    loss_seq = []\n",
    "    for epoch in range(epochs):\n",
    "        avg_loss = train_one_epoch(encoder, decoder, X_features, Y_meals, target_EIs, min_macros, max_macros, batch_size, optimizer)\n",
    "        if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "            loss_seq.append(avg_loss)\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Avg Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return encoder, decoder, loss_seq"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
